{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Information\n",
    "Name: 丁浩文\n",
    "\n",
    "Student ID: 109062517\n",
    "\n",
    "GitHub ID: joshua049\n",
    "\n",
    "Kaggle name: Hao-Wen Ting\n",
    "\n",
    "Kaggle private scoreboard snapshot:\n",
    "\n",
    "![](https://imgur.com/OCktU1I.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "## Preprocessing\n",
    "I have tried several methods to improve my performance in this assighnment. Before training and testing, I preprocess the data first so I can directly use these preprocessed data to build my model.\n",
    "\n",
    "I read *tweets_DM.json* first, and I create a DataFrame with columns to maintain the tweet ids and corresponding articles on twitter. In order to clear up the data, I use *re* and *BeautifulSoup* package to remove useless informations such as HTML tags and emoticons. I also read *emotion.csv* and *data_identification.csv* as DataFrames. After that, I merged the three DataFrames on *tweet_id* and apply tokenizer of *nltk* package on the content column to remove stop words, so I can check all needed attributes in each record by query *tweet_id*. \n",
    "\n",
    "Finally, I use the *identification* column to recognize if the record is training data or not. Furthurmore, I also use *train_test_split* from *sklearn.model_selection* to split a validation set to validate my model's performance during the training process.\n",
    "\n",
    "## Training\n",
    "What I have tried can be divied into two parts: **traditional machine learning-based method** and **deep learning**. \n",
    "\n",
    "### Traditional Machine Learning\n",
    "In this part, I use *CountVectorizer* and *TfidfVectorizer* to extract the matrix as feature vectors. And then I tried several classifiers to compare their performances. After several times of experiment, I found that most classifiers could achieve a certain level of performance, except NuSVC and LinearSVC. Hence I use classifiers with better performance as base estimators, and use the StackingClassifier to combine them and enhance the overall performance.\n",
    "\n",
    "Validation results of models I have tried:\n",
    "\n",
    "|       Classifier       | f1-score |\n",
    "| :--------------------: | :------: |\n",
    "|     MultinomialNB      |   0.53   |\n",
    "|   LogisticRegression   |   0.51   |\n",
    "| RandomForestClassifier |   0.49   |\n",
    "|     XGBClassifier      |   0.47   |\n",
    "|         NuSVC          |   0.37   |\n",
    "|       LinearSVC        |   0.39   |\n",
    "|     LGBMClassifier     |   0.49   |\n",
    "|   StackingClassifier   |   0.60   |\n",
    "\n",
    "\n",
    "### Deep Learning\n",
    "In this part, I tried to train the model in a recurrent way. I use the bert model released by Google as my backbone, then add two fully-connected layers to output 8 channels as classification result. During the training process, I set the weights of the backbone to be non-trainable, so that I can finetune the weights in fully-connected layers properly. \n",
    "\n",
    "However, due to lack of time and powerful training resources, I could only sample 40000 training data to make my model converge.This method only get 0.448 in f1-score, which doesn't exceed that of my stacking classifier. If I had started this experiment earlier, I think it would achieve a better performance.\n",
    "\n",
    "## Output\n",
    "During the training process, I have used label encoder for both methods. Hence both methods would output a number to represent its classification result, then I can reuse the label encoder to decode the number into the class it represented.\n",
    "\n",
    "## Results\n",
    "According to my best validation result, my testing score should be around 0.60. However, I only got 0.46 in private leaderboard. I think it's because I didn't do cross validation, so my validation should exist some bias between the test set, which leads to the huge difference of performance. Next time I should do cross validation to ensure my model could get excellent performance under all circumstances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import bert\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer\n",
    "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data'\n",
    "\n",
    "id_data = pd.read_csv(os.path.join(data_dir, 'data_identification.csv'))\n",
    "emotion_data = pd.read_csv(os.path.join(data_dir, 'emotion.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'tweets_DM.json')) as f:\n",
    "    all_content = f.read().splitlines()\n",
    "len(all_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweetids = []\n",
    "all_scores = []\n",
    "all_hashtags = []\n",
    "all_textwords = []\n",
    "for line in tqdm(all_content):\n",
    "    content = eval(line.strip())        \n",
    "\n",
    "    score = content['_score']\n",
    "    hashtag = content['_source']['tweet']['hashtags']\n",
    "    tweetid = content['_source']['tweet']['tweet_id']\n",
    "\n",
    "    text = content['_source']['tweet']['text']       \n",
    "\n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "\n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "\n",
    "\n",
    "    all_tweetids.append(tweetid)\n",
    "    all_scores.append(score)\n",
    "    all_hashtags.append(hashtag)\n",
    "    all_textwords.append(text)\n",
    "        \n",
    "content_data = pd.DataFrame({'tweet_id': all_tweetids, 'score': all_scores, 'hashtag': all_hashtags, 'words': all_textwords})\n",
    "content_data.to_csv(os.path.join(data_dir, 'content.csv'), index=None, encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.merge(content_data, id_data, on='tweet_id')\n",
    "all_data['text_tokenized'] = all_data['words'].apply(lambda x: nltk.word_tokenize(x))\n",
    "# all_data.to_csv(os.path.join(data_dir, 'all.csv'), index=None, columns=['tweet_id', 'score', 'hashtag', 'words', 'text_tokenized', 'identification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.merge(all_data, emotion_data, on='tweet_id')\n",
    "testing_data = all_data[all_data['identification']=='test']\n",
    "# training_data = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n",
    "# testing_data = pd.read_csv(os.path.join(data_dir, 'test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = training_data.emotion\n",
    "training_data = training_data.drop(['emotion'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "target = le.fit_transform(target)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_acc(model):\n",
    "    predicted = model.predict(X_test.words)\n",
    "    accuracy = np.mean(predicted == y_test) * 100\n",
    "    print(f1_score(y_test, predicted, average='macro'))\n",
    "    print(f1_score(y_test, predicted, average='micro'))\n",
    "    print(f1_score(y_test, predicted, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_list = [\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(),\n",
    "    RandomForestClassifier(),\n",
    "    XGBClassifier(),\n",
    "    NuSVC(),\n",
    "    LinearSVC(),\n",
    "    LGBMClassifier()\n",
    "]\n",
    "\n",
    "for clf in clf_list:\n",
    "    nb_clf = Pipeline([('vect', CountVectorizer()), ('clf', clf)])\n",
    "    nb_clf = nb_clf.fit(X_train.words,y_train)\n",
    "    print_acc(nb_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [\n",
    "    ('xgb', XGBClassifier(n_estimators=120, max_depth=11, learning_rate=0.1)),\n",
    "    ('lgb', LGBMClassifier(max_depth=4, num_leaves=20)),\n",
    "    ('nb', MultinomialNB()),\n",
    "    ('rf', RandomForestClassifier(n_estimators=120, max_depth=11))\n",
    "]\n",
    "\n",
    "start = time.time()\n",
    "nb_clf = Pipeline([('vect', CountVectorizer(stop_words=stop_words, dtype=np.float32)), ('clf', StackingClassifier(estimators=estimators, final_estimator=LogisticRegression()))])\n",
    "nb_clf = nb_clf.fit(X_train.content,y_train)\n",
    "print_acc(nb_clf)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = le.classes_[nb_clf.predict(testing_data.words)]\n",
    "output_df = pd.DataFrame({'id': test_data.tweet_id, 'emotion': output})\n",
    "output_df.to_csv('output.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Structure of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionalData:\n",
    "    DATA_COLUMN = \"words\"\n",
    "    LABEL_COLUMN = \"emotion\"\n",
    "    \n",
    "    def __init__(self, df, le, tokenizer: FullTokenizer, test=False, max_seq_len=1024, sample_size=None, ratio_dict=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = 0\n",
    "        self.le = le\n",
    "        \n",
    "        if not test:        \n",
    "            if sample_size is not None:\n",
    "                # train, test = map(lambda df: df.sample(sample_size), [train, test])\n",
    "                if ratio_dict is not None:\n",
    "#                     N = (sample_size // len(self.le.classes_)) + 1\n",
    "                    dfs = [df[df[self.LABEL_COLUMN]==key].sample(int(sample_size * val)) for key, val in ratio_dict.items()]\n",
    "                    df = pd.concat(dfs)             \n",
    "                else:\n",
    "                    df = df.sample(sample_size)\n",
    "                df = shuffle(df)\n",
    "            self.train_x, self.train_y = self._prepare(df)            \n",
    "            print(\"max seq_len\", self.max_seq_len)\n",
    "            self.max_seq_len = max_seq_len    \n",
    "            self.train_x, self.train_x_token_types = self._pad(self.train_x)\n",
    "        else:            \n",
    "            self.test_x = self._test_prepare(df)\n",
    "            self.max_seq_len = max_seq_len\n",
    "            self.test_x, self.test_x_token_types = self._pad(self.test_x)\n",
    "            \n",
    "\n",
    "    def _prepare(self, df):\n",
    "        x, y = [], []\n",
    "        with tqdm(total=df.shape[0], unit_scale=True) as pbar:\n",
    "            for ndx, row in df.iterrows():\n",
    "                text, label = row[EmotionalData.DATA_COLUMN], row[EmotionalData.LABEL_COLUMN]\n",
    "                tokens = self.tokenizer.tokenize(text)\n",
    "                tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "                token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "                self.max_seq_len = max(self.max_seq_len, len(token_ids))\n",
    "                x.append(token_ids)\n",
    "                y.append(label)\n",
    "                pbar.update()\n",
    "        return np.array(x), self.le.transform(np.array(y))\n",
    "    \n",
    "    def _test_prepare(self, df):\n",
    "        x = []\n",
    "        \n",
    "        for ndx, row in tqdm(df.iterrows()):\n",
    "            text = row[EmotionalData.DATA_COLUMN]\n",
    "            tokens = self.tokenizer.tokenize(text)\n",
    "            tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "            token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "            self.max_seq_len = max(self.max_seq_len, len(token_ids))\n",
    "            x.append(token_ids)\n",
    "            \n",
    "        return np.array(x)\n",
    "\n",
    "    def _pad(self, ids):\n",
    "        x, t = [], []\n",
    "        token_type_ids = [0] * self.max_seq_len\n",
    "        for input_ids in tqdm(ids):\n",
    "            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n",
    "            input_ids = list(input_ids) + list([0] * (self.max_seq_len - len(input_ids)))\n",
    "            x.append(np.array(input_ids))\n",
    "            t.append(token_type_ids)\n",
    "        return np.array(x), np.array(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc = OneHotEncoder(handle_unknown='ignore')\n",
    "# X, y = training_data.words, enc.fit_transform(training_data.emotion.to_numpy().reshape(-1, 1))\n",
    "\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"../../../repos/bert/cased_L-12_H-768_A-12\"\n",
    "\n",
    "bert_params = bert.params_from_pretrained_ckpt(model_dir)\n",
    "l_bert = bert.BertModelLayer.from_params(bert_params, name=\"bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter(training_data.emotion)\n",
    "new_dict = {key: val**0.42 for key, val in c.items()}\n",
    "N = sum(new_dict.values())\n",
    "ratio_dict = {key: val/N for key, val in new_dict.items()}\n",
    "\n",
    "tokenizer = FullTokenizer(vocab_file=os.path.join(model_dir, \"vocab.txt\"))\n",
    "le = LabelEncoder().fit(training_data.emotion)\n",
    "train_data = EmotionalData(training_data, le, tokenizer, max_seq_len=128, sample_size=45000, ratio_dict=ratio_dict)\n",
    "test_data = EmotionalData(testing_data, le, tokenizer, test=True, max_seq_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"            train_x\", train_data.train_x.shape)\n",
    "print(\"train_x_token_types\", train_data.train_x_token_types.shape)\n",
    "print(\"            train_y\", train_data.train_y.shape)\n",
    "print(\"        max_seq_len\", train_data.max_seq_len)\n",
    "print(\"             test_x\", test_data.test_x.shape)\n",
    "print(\"        max_seq_len\", test_data.max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_layers(root_layer):\n",
    "    if isinstance(root_layer, tf.keras.layers.Layer):\n",
    "        yield root_layer\n",
    "    for layer in root_layer._layers:\n",
    "        for sub_layer in flatten_layers(layer):\n",
    "            yield sub_layer\n",
    "\n",
    "\n",
    "def freeze_bert_layers(l_bert):\n",
    "    \"\"\"\n",
    "    Freezes all but LayerNorm and adapter layers - see arXiv:1902.00751.\n",
    "    \"\"\"\n",
    "    for layer in flatten_layers(l_bert):\n",
    "        if layer.name in [\"LayerNorm\", \"adapter-down\", \"adapter-up\"]:\n",
    "            layer.trainable = True\n",
    "        elif len(layer._layers) == 0:\n",
    "            layer.trainable = False\n",
    "        l_bert.embeddings_layer.trainable = False\n",
    "\n",
    "\n",
    "def create_learning_rate_scheduler(max_learn_rate=5e-5,\n",
    "                                   end_learn_rate=1e-7,\n",
    "                                   warmup_epoch_count=10,\n",
    "                                   total_epoch_count=90):\n",
    "\n",
    "    def lr_scheduler(epoch):\n",
    "        if epoch < warmup_epoch_count:\n",
    "            res = (max_learn_rate/warmup_epoch_count) * (epoch + 1)\n",
    "        else:\n",
    "            res = max_learn_rate*math.exp(math.log(end_learn_rate/max_learn_rate)*(epoch-warmup_epoch_count+1)/(total_epoch_count-warmup_epoch_count+1))\n",
    "        return float(res)\n",
    "    learning_rate_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n",
    "\n",
    "    return learning_rate_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config_file = os.path.join(model_dir, \"bert_config.json\")\n",
    "bert_ckpt_file   = os.path.join(model_dir, \"bert_model.ckpt\")\n",
    "\n",
    "def create_model(max_seq_len, adapter_size=64):\n",
    "    \"\"\"Creates a classification model.\"\"\"\n",
    "\n",
    "    #adapter_size = 64  # see - arXiv:1902.00751\n",
    "\n",
    "    # create the bert layer\n",
    "    with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n",
    "        bc = StockBertConfig.from_json_string(reader.read())\n",
    "        bert_params = map_stock_config_to_params(bc)\n",
    "        bert_params.adapter_size = adapter_size\n",
    "        l_bert = bert.BertModelLayer.from_params(bert_params, name=\"bert\")\n",
    "\n",
    "    input_ids      = tf.keras.layers.Input(shape=(max_seq_len,), dtype='int32', name=\"input_ids\")\n",
    "    # token_type_ids = keras.layers.Input(shape=(max_seq_len,), dtype='int32', name=\"token_type_ids\")\n",
    "    # output         = bert([input_ids, token_type_ids])\n",
    "    output         = l_bert(input_ids)\n",
    "\n",
    "    print(\"bert shape\", output.shape)\n",
    "    cls_out = tf.keras.layers.Lambda(lambda seq: seq[:, 0, :])(output)\n",
    "    cls_out = tf.keras.layers.Dropout(0.5)(cls_out)\n",
    "    logits = tf.keras.layers.Dense(units=768, activation=\"tanh\")(cls_out)\n",
    "    logits = tf.keras.layers.Dropout(0.5)(logits)\n",
    "    logits = tf.keras.layers.Dense(units=len(le.classes_), activation=\"softmax\")(logits)\n",
    "\n",
    "    # model = keras.Model(inputs=[input_ids, token_type_ids], outputs=logits)\n",
    "    # model.build(input_shape=[(None, max_seq_len), (None, max_seq_len)])\n",
    "    model = tf.keras.Model(inputs=input_ids, outputs=logits)\n",
    "    model.build(input_shape=(None, max_seq_len))\n",
    "\n",
    "    # load the pre-trained model weights\n",
    "    load_stock_weights(l_bert, bert_ckpt_file)\n",
    "\n",
    "    # freeze weights if adapter-BERT is used\n",
    "#     if adapter_size is not None:\n",
    "    freeze_bert_layers(l_bert)\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"acc\")])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_size = None # use None to fine-tune all of BERT\n",
    "model = create_model(train_data.max_seq_len, adapter_size=adapter_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('./emotion.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "log_dir = \"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%s\")\n",
    "os.mkdir(log_dir)\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "\n",
    "total_epoch_count = 50\n",
    "# model.fit(x=(data.train_x, data.train_x_token_types), y=data.train_y,\n",
    "model.fit(x=train_data.train_x, y=train_data.train_y,\n",
    "          validation_split=0.2,\n",
    "          batch_size=8,\n",
    "          shuffle=True,\n",
    "          epochs=total_epoch_count,\n",
    "          callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_lr=1e-7),\n",
    "                     tf.keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True),                   \n",
    "                     tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./emotion_30000_finetune.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def prediction_step(input_):\n",
    "    return model(input_, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_output(confidences):\n",
    "    return confidences.numpy().argmax()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list = []\n",
    "for input_ in tqdm(test_data.test_x):\n",
    "    class_num = process_output(prediction_step(tf.expand_dims(input_, axis=0)))\n",
    "    output_list.append(le.classes_[class_num])\n",
    "    \n",
    "output_df = pd.DataFrame({'id': testing_data.tweet_id, 'emotion': output_list})\n",
    "output_df.to_csv('output.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c dm2020-hw2-nthu -f output.csv -m \"try bert balanced 0.4\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
